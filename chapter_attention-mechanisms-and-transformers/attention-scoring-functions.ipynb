{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b6879b5d",
      "metadata": {
        "id": "b6879b5d"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "62ae200b",
      "metadata": {
        "id": "62ae200b",
        "outputId": "4ebaea09-0a90-4ee7-fb4e-6823e27d193f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l\n",
            "  Using cached d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: d2l\n",
            "Successfully installed d2l-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l --no-deps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efaabde6",
      "metadata": {
        "origin_pos": 1,
        "id": "efaabde6"
      },
      "source": [
        "# Attention Scoring Functions\n",
        ":label:`sec_attention-scoring-functions`\n",
        "\n",
        "\n",
        "In :numref:`sec_attention-pooling`,\n",
        "we used a number of different distance-based kernels, including a Gaussian kernel to model\n",
        "interactions between queries and keys. As it turns out, distance functions are slightly more expensive to compute than dot products. As such,\n",
        "with the softmax operation to ensure nonnegative attention weights,\n",
        "much of the work has gone into *attention scoring functions* $a$ in :eqref:`eq_softmax_attention` and :numref:`fig_attention_output` that are simpler to compute.\n",
        "\n",
        "![Computing the output of attention pooling as a weighted average of values, where weights are computed with the attention scoring function $\\mathit{a}$ and the softmax operation.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/attention-output.svg?raw=1)\n",
        ":label:`fig_attention_output`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8e33a108",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:45.433072Z",
          "iopub.status.busy": "2023-08-18T19:43:45.432523Z",
          "iopub.status.idle": "2023-08-18T19:43:48.504425Z",
          "shell.execute_reply": "2023-08-18T19:43:48.503548Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "8e33a108"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8224956",
      "metadata": {
        "origin_pos": 6,
        "id": "a8224956"
      },
      "source": [
        "## [**Dot Product Attention**]\n",
        "\n",
        "\n",
        "Let's review the attention function (without exponentiation) from the Gaussian kernel for a moment:\n",
        "\n",
        "$$\n",
        "a(\\mathbf{q}, \\mathbf{k}_i) = -\\frac{1}{2} \\|\\mathbf{q} - \\mathbf{k}_i\\|^2  = \\mathbf{q}^\\top \\mathbf{k}_i -\\frac{1}{2} \\|\\mathbf{k}_i\\|^2  -\\frac{1}{2} \\|\\mathbf{q}\\|^2.\n",
        "$$\n",
        "\n",
        "First, note that the final term depends on $\\mathbf{q}$ only. As such it is identical for all $(\\mathbf{q}, \\mathbf{k}_i)$ pairs. Normalizing the attention weights to $1$, as is done in :eqref:`eq_softmax_attention`, ensures that this term disappears entirely. Second, note that both batch and layer normalization (to be discussed later) lead to activations that have well-bounded, and often constant, norms $\\|\\mathbf{k}_i\\|$. This is the case, for instance, whenever the keys $\\mathbf{k}_i$ were generated by a layer norm. As such, we can drop it from the definition of $a$ without any major change in the outcome.\n",
        "\n",
        "Last, we need to keep the order of magnitude of the arguments in the exponential function under control. Assume that all the elements of the query $\\mathbf{q} \\in \\mathbb{R}^d$ and the key $\\mathbf{k}_i \\in \\mathbb{R}^d$ are independent and identically drawn random variables with zero mean and unit variance. The dot product between both vectors has zero mean and a variance of $d$. To ensure that the variance of the dot product still remains $1$ regardless of vector length, we use the *scaled dot product attention* scoring function. That is, we rescale the dot product by $1/\\sqrt{d}$. We thus arrive at the first commonly used attention function that is used, e.g., in Transformers :cite:`Vaswani.Shazeer.Parmar.ea.2017`:\n",
        "\n",
        "$$ a(\\mathbf{q}, \\mathbf{k}_i) = \\mathbf{q}^\\top \\mathbf{k}_i / \\sqrt{d}.$$\n",
        ":eqlabel:`eq_dot_product_attention`\n",
        "\n",
        "Note that attention weights $\\alpha$ still need normalizing. We can simplify this further via :eqref:`eq_softmax_attention` by using the softmax operation:\n",
        "\n",
        "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(\\mathbf{q}^\\top \\mathbf{k}_i / \\sqrt{d})}{\\sum_{j=1} \\exp(\\mathbf{q}^\\top \\mathbf{k}_j / \\sqrt{d})}.$$\n",
        ":eqlabel:`eq_attn-scoring-alpha`\n",
        "\n",
        "As it turns out, all popular attention mechanisms use the softmax, hence we will limit ourselves to that in the remainder of this chapter.\n",
        "\n",
        "## Convenience Functions\n",
        "\n",
        "We need a few functions to make the attention mechanism efficient to deploy. This includes tools for dealing with strings of variable lengths (common for natural language processing) and tools for efficient evaluation on minibatches (batch matrix multiplication).\n",
        "\n",
        "\n",
        "### [**Masked Softmax Operation**]\n",
        "\n",
        "One of the most popular applications of the attention mechanism is to sequence models. Hence we need to be able to deal with sequences of different lengths. In some cases, such sequences may end up in the same minibatch, necessitating padding with dummy tokens for shorter sequences (see :numref:`sec_machine_translation` for an example). These special tokens do not carry meaning. For instance, assume that we have the following three sentences:\n",
        "\n",
        "```\n",
        "Dive  into  Deep    Learning\n",
        "Learn to    code    <blank>\n",
        "Hello world <blank> <blank>\n",
        "```\n",
        "\n",
        "Since we do not want blanks in our attention model we simply need to limit $\\sum_{i=1}^n \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i$ to $\\sum_{i=1}^l \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i$ for however long, $l \\leq n$, the actual sentence is. Since it is such a common problem, it has a name: the *masked softmax operation*.\n",
        "\n",
        "Let's implement it. Actually, the implementation cheats ever so slightly by setting the values of $\\mathbf{v}_i$, for $i > l$, to zero. Moreover, it sets the attention weights to a large negative number, such as $-10^{6}$, in order to make their contribution to gradients and values vanish in practice. This is done since linear algebra kernels and operators are heavily optimized for GPUs and it is faster to be slightly wasteful in computation rather than to have code with conditional (if then else) statements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "080c4919",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.508521Z",
          "iopub.status.busy": "2023-08-18T19:43:48.507880Z",
          "iopub.status.idle": "2023-08-18T19:43:48.515032Z",
          "shell.execute_reply": "2023-08-18T19:43:48.514260Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "080c4919"
      },
      "outputs": [],
      "source": [
        "def masked_softmax(X, valid_lens):\n",
        "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
        "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
        "    def _sequence_mask(X, valid_len, value=0):\n",
        "        maxlen = X.size(1)\n",
        "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                            device=X.device)[None, :] < valid_len[:, None]\n",
        "        X[~mask] = value\n",
        "        return X\n",
        "\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        # On the last axis, replace masked elements with a very large negative\n",
        "        # value, whose exponentiation outputs 0\n",
        "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac9441c0",
      "metadata": {
        "origin_pos": 11,
        "id": "ac9441c0"
      },
      "source": [
        "To [**illustrate how this function works**],\n",
        "consider a minibatch of two examples of size $2 \\times 4$,\n",
        "where their valid lengths are $2$ and $3$, respectively.\n",
        "As a result of the masked softmax operation,\n",
        "values beyond the valid lengths for each pair of vectors are all masked as zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b0fb493b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.518456Z",
          "iopub.status.busy": "2023-08-18T19:43:48.517778Z",
          "iopub.status.idle": "2023-08-18T19:43:48.554108Z",
          "shell.execute_reply": "2023-08-18T19:43:48.553283Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "b0fb493b",
        "outputId": "0a224a0b-9a06-4256-dbe9-437028bd6223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4775, 0.5225, 0.0000, 0.0000],\n",
              "         [0.4447, 0.5553, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.3655, 0.3235, 0.3110, 0.0000],\n",
              "         [0.3580, 0.2395, 0.4024, 0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390fd5a8",
      "metadata": {
        "origin_pos": 16,
        "id": "390fd5a8"
      },
      "source": [
        "If we need more fine-grained control to specify the valid length for each of the two vectors of every example, we simply use a two-dimensional tensor of valid lengths. This yields:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0eff10c9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.557828Z",
          "iopub.status.busy": "2023-08-18T19:43:48.557262Z",
          "iopub.status.idle": "2023-08-18T19:43:48.564098Z",
          "shell.execute_reply": "2023-08-18T19:43:48.563239Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "0eff10c9",
        "outputId": "232c10d2-3f11-40a4-e0bf-5b4eba475f93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3240, 0.3628, 0.3132, 0.0000]],\n",
              "\n",
              "        [[0.4942, 0.5058, 0.0000, 0.0000],\n",
              "         [0.2754, 0.2097, 0.2277, 0.2872]]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03f10da",
      "metadata": {
        "origin_pos": 21,
        "id": "a03f10da"
      },
      "source": [
        "### Batch Matrix Multiplication\n",
        ":label:`subsec_batch_dot`\n",
        "\n",
        "Another commonly used operation is to multiply batches of matrices by one another. This comes in handy when we have minibatches of queries, keys, and values. More specifically, assume that\n",
        "\n",
        "$$\\mathbf{Q} = [\\mathbf{Q}_1, \\mathbf{Q}_2, \\ldots, \\mathbf{Q}_n]  \\in \\mathbb{R}^{n \\times a \\times b}, \\\\\n",
        "    \\mathbf{K} = [\\mathbf{K}_1, \\mathbf{K}_2, \\ldots, \\mathbf{K}_n]  \\in \\mathbb{R}^{n \\times b \\times c}.\n",
        "$$\n",
        "\n",
        "Then the batch matrix multiplication (BMM) computes the elementwise product\n",
        "\n",
        "$$\\textrm{BMM}(\\mathbf{Q}, \\mathbf{K}) = [\\mathbf{Q}_1 \\mathbf{K}_1, \\mathbf{Q}_2 \\mathbf{K}_2, \\ldots, \\mathbf{Q}_n \\mathbf{K}_n] \\in \\mathbb{R}^{n \\times a \\times c}.$$\n",
        ":eqlabel:`eq_batch-matrix-mul`\n",
        "\n",
        "Let's see this in action in a deep learning framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1d592456",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.567605Z",
          "iopub.status.busy": "2023-08-18T19:43:48.567037Z",
          "iopub.status.idle": "2023-08-18T19:43:48.572146Z",
          "shell.execute_reply": "2023-08-18T19:43:48.571131Z"
        },
        "origin_pos": 23,
        "tab": [
          "pytorch"
        ],
        "id": "1d592456"
      },
      "outputs": [],
      "source": [
        "Q = torch.ones((2, 3, 4))\n",
        "K = torch.ones((2, 4, 6))\n",
        "d2l.check_shape(torch.bmm(Q, K), (2, 3, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aef7274",
      "metadata": {
        "origin_pos": 26,
        "id": "4aef7274"
      },
      "source": [
        "## [**Scaled Dot Product Attention**]\n",
        "\n",
        "Let's return to the dot product attention introduced in :eqref:`eq_dot_product_attention`.\n",
        "In general, it requires that both the query and the key\n",
        "have the same vector length, say $d$, even though this can be addressed easily by replacing\n",
        "$\\mathbf{q}^\\top \\mathbf{k}$ with $\\mathbf{q}^\\top \\mathbf{M} \\mathbf{k}$ where $\\mathbf{M}$ is a matrix suitably chosen for translating between both spaces. For now assume that the dimensions match.\n",
        "\n",
        "In practice, we often think of minibatches for efficiency,\n",
        "such as computing attention for $n$ queries and $m$ key-value pairs,\n",
        "where queries and keys are of length $d$\n",
        "and values are of length $v$. The scaled dot product attention\n",
        "of queries $\\mathbf Q\\in\\mathbb R^{n\\times d}$,\n",
        "keys $\\mathbf K\\in\\mathbb R^{m\\times d}$,\n",
        "and values $\\mathbf V\\in\\mathbb R^{m\\times v}$\n",
        "thus can be written as\n",
        "\n",
        "$$ \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}.$$\n",
        ":eqlabel:`eq_softmax_QK_V`\n",
        "\n",
        "Note that when applying this to a minibatch, we need the batch matrix multiplication introduced in :eqref:`eq_batch-matrix-mul`. In the following implementation of the scaled dot product attention,\n",
        "we use dropout for model regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "33207d5f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.575743Z",
          "iopub.status.busy": "2023-08-18T19:43:48.575036Z",
          "iopub.status.idle": "2023-08-18T19:43:48.581055Z",
          "shell.execute_reply": "2023-08-18T19:43:48.580209Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "33207d5f"
      },
      "outputs": [],
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    \"\"\"Scaled dot product attention.\"\"\"\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Shape of queries: (batch_size, no. of queries, d)\n",
        "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
        "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
        "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        d = queries.shape[-1]\n",
        "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb8df37",
      "metadata": {
        "origin_pos": 31,
        "id": "dbb8df37"
      },
      "source": [
        "To [**illustrate how the `DotProductAttention` class works**],\n",
        "we use the same keys, values, and valid lengths from the earlier toy example for additive attention. For the purpose of our example we assume that we have a minibatch size of $2$, a total of $10$ keys and values, and that the dimensionality of the values is $4$. Lastly, we assume that the valid length per observation is $2$ and $6$ respectively. Given that, we expect the output to be a $2 \\times 1 \\times 4$ tensor, i.e., one row per example of the minibatch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2f449209",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.584794Z",
          "iopub.status.busy": "2023-08-18T19:43:48.584072Z",
          "iopub.status.idle": "2023-08-18T19:43:48.590854Z",
          "shell.execute_reply": "2023-08-18T19:43:48.589996Z"
        },
        "origin_pos": 33,
        "tab": [
          "pytorch"
        ],
        "id": "2f449209"
      },
      "outputs": [],
      "source": [
        "queries = torch.normal(0, 1, (2, 1, 2))\n",
        "keys = torch.normal(0, 1, (2, 10, 2))\n",
        "values = torch.normal(0, 1, (2, 10, 4))\n",
        "valid_lens = torch.tensor([2, 6])\n",
        "\n",
        "attention = DotProductAttention(dropout=0.5)\n",
        "attention.eval()\n",
        "d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00e17255",
      "metadata": {
        "origin_pos": 36,
        "id": "00e17255"
      },
      "source": [
        "Let's check whether the attention weights actually vanish for anything beyond the second and sixth column respectively (because of setting the valid length to $2$ and $6$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f40e370d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.594461Z",
          "iopub.status.busy": "2023-08-18T19:43:48.593898Z",
          "iopub.status.idle": "2023-08-18T19:43:48.969221Z",
          "shell.execute_reply": "2023-08-18T19:43:48.968308Z"
        },
        "origin_pos": 37,
        "tab": [
          "pytorch"
        ],
        "id": "f40e370d",
        "outputId": "e0443de1-7a1e-4fae-c099-3b21cd0a81c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 250x250 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"187.07675pt\" height=\"103.438906pt\" viewBox=\"0 0 187.07675 103.438906\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-09-04T06:51:26.971468</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 103.438906 \nL 187.07675 103.438906 \nL 187.07675 0 \nL -0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 59.94 \nL 145.840625 59.94 \nL 145.840625 37.62 \nL 34.240625 37.62 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pfe2ad2028e)\">\n    <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAJsAAAAfCAYAAADwQL9CAAAAtUlEQVR4nO3asQnCQBiG4TtttLcRBNdyDsdwgmzgOC4RCART2gm6wx18Enye/uO/4iVV6vt++5RWu33ztJRSyuHYvn1Ofbdrbd8uc9fp7eXatV+rza8fwP8QGzFiI0ZsxIiNGLERIzZixEaM2IgRGzFiI0ZsxIiNGLERIzZi6uN0bv6fbRiXruPDa+zasy6+bMSIjRixESM2YsRGjNiIERsxYiNGbMSIjRixESM2YsRGjNiI+QJt3w8DS4Dp1QAAAABJRU5ErkJggg==\" id=\"image4749c0b3ac\" transform=\"scale(1 -1) translate(0 -22.32)\" x=\"34.240625\" y=\"-37.62\" width=\"111.6\" height=\"22.32\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m1e49d339ff\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m1e49d339ff\" x=\"39.820625\" y=\"59.94\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(36.639375 74.538438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m1e49d339ff\" x=\"95.620625\" y=\"59.94\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(92.439375 74.538438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Keys -->\n     <g transform=\"translate(78.371094 88.216563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 2694 \nL 3353 4666 \nL 4166 4666 \nL 1850 2491 \nL 4331 0 \nL 3500 0 \nL 1259 2247 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4b\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(60.576172 0)\"/>\n      <use xlink:href=\"#DejaVuSans-79\" transform=\"translate(122.099609 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(181.279297 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path id=\"m083adee27c\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m083adee27c\" x=\"34.240625\" y=\"43.2\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(20.878125 46.999219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m083adee27c\" x=\"34.240625\" y=\"54.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(20.878125 58.159219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Queries -->\n     <g transform=\"translate(14.798437 68.087031) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-51\" d=\"M 2522 4238 \nQ 1834 4238 1429 3725 \nQ 1025 3213 1025 2328 \nQ 1025 1447 1429 934 \nQ 1834 422 2522 422 \nQ 3209 422 3611 934 \nQ 4013 1447 4013 2328 \nQ 4013 3213 3611 3725 \nQ 3209 4238 2522 4238 \nz\nM 3406 84 \nL 4238 -825 \nL 3475 -825 \nL 2784 -78 \nQ 2681 -84 2626 -87 \nQ 2572 -91 2522 -91 \nQ 1538 -91 948 567 \nQ 359 1225 359 2328 \nQ 359 3434 948 4092 \nQ 1538 4750 2522 4750 \nQ 3503 4750 4090 4092 \nQ 4678 3434 4678 2328 \nQ 4678 1516 4351 937 \nQ 4025 359 3406 84 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-51\"/>\n      <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(78.710938 0)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(142.089844 0)\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(203.613281 0)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(244.726562 0)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(272.509766 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(334.033203 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 59.94 \nL 34.240625 37.62 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 145.840625 59.94 \nL 145.840625 37.62 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 59.94 \nL 145.840625 59.94 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 37.62 \nL 145.840625 37.62 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 152.815625 90.36 \nL 156.973625 90.36 \nL 156.973625 7.2 \nL 152.815625 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAYAAAB0CAYAAACmJkOCAAAAyElEQVR4nM2WSwrEMAxDXcj9zzqbblp/egG/gEII02WEJMt2Qq+6f2XNN6yyO7dhGQQQo1qLFY9ij30AB5Rz6B7uIqNQKkiKAUwuAyRV8e6SmjTxgAcy5An+qTkD+vrQNVgw15sI5qWXi909kLwCpTCgDPgB83dfuXpVFDAD3vakR18H3KEqWJKJVCBAyTEgSukBw2WP9thsJP0zOAEoFcjoz22EUQ5kyFVhwH5BZx4MkBQ2Ee7mhAEjX2E88sxRCluyYg4MquoD0XoQUGMmzEcAAAAASUVORK5CYII=\" id=\"imagec6888899af\" transform=\"scale(1 -1) translate(0 -83.52)\" x=\"152.64\" y=\"-6.48\" width=\"4.32\" height=\"83.52\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path id=\"m984b9c69d4\" d=\"M 0 0 \nL 3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m984b9c69d4\" x=\"156.973625\" y=\"90.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(163.973625 94.159219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m984b9c69d4\" x=\"156.973625\" y=\"61.569407\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(163.973625 65.368626) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m984b9c69d4\" x=\"156.973625\" y=\"32.778814\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(163.973625 36.578033) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-34\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 152.815625 90.36 \nL 154.894625 90.36 \nL 156.973625 90.36 \nL 156.973625 7.2 \nL 154.894625 7.2 \nL 152.815625 7.2 \nL 152.815625 90.36 \nz\n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pfe2ad2028e\">\n   <rect x=\"34.240625\" y=\"37.62\" width=\"111.6\" height=\"22.32\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
        "                  xlabel='Keys', ylabel='Queries')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583e7b8d",
      "metadata": {
        "origin_pos": 39,
        "id": "583e7b8d"
      },
      "source": [
        "## [**Additive Attention**]\n",
        ":label:`subsec_additive-attention`\n",
        "\n",
        "When queries $\\mathbf{q}$ and keys $\\mathbf{k}$ are vectors of different dimension,\n",
        "we can either use a matrix to address the mismatch via $\\mathbf{q}^\\top \\mathbf{M} \\mathbf{k}$, or we can use additive attention\n",
        "as the scoring function. Another benefit is that, as its name indicates, the attention is additive. This can lead to some minor computational savings.\n",
        "Given a query $\\mathbf{q} \\in \\mathbb{R}^q$\n",
        "and a key $\\mathbf{k} \\in \\mathbb{R}^k$,\n",
        "the *additive attention* scoring function :cite:`Bahdanau.Cho.Bengio.2014` is given by\n",
        "\n",
        "$$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\textrm{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$\n",
        ":eqlabel:`eq_additive-attn`\n",
        "\n",
        "where $\\mathbf W_q\\in\\mathbb R^{h\\times q}$, $\\mathbf W_k\\in\\mathbb R^{h\\times k}$,\n",
        "and $\\mathbf w_v\\in\\mathbb R^{h}$ are the learnable parameters. This term is then fed into a softmax to ensure both nonnegativity and normalization.\n",
        "An equivalent interpretation of :eqref:`eq_additive-attn` is that the query and key are concatenated\n",
        "and fed into an MLP with a single hidden layer.\n",
        "Using $\\tanh$ as the activation function and disabling bias terms,\n",
        "we implement additive attention as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3a2e6dee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.973108Z",
          "iopub.status.busy": "2023-08-18T19:43:48.972388Z",
          "iopub.status.idle": "2023-08-18T19:43:48.979819Z",
          "shell.execute_reply": "2023-08-18T19:43:48.978914Z"
        },
        "origin_pos": 41,
        "tab": [
          "pytorch"
        ],
        "id": "3a2e6dee"
      },
      "outputs": [],
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    \"\"\"Additive attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, dropout, **kwargs):\n",
        "        super(AdditiveAttention, self).__init__(**kwargs)\n",
        "        self.W_k = nn.LazyLinear(num_hiddens, bias=False)\n",
        "        self.W_q = nn.LazyLinear(num_hiddens, bias=False)\n",
        "        self.w_v = nn.LazyLinear(1, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
        "        # After dimension expansion, shape of queries: (batch_size, no. of\n",
        "        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n",
        "        # key-value pairs, num_hiddens). Sum them up with broadcasting\n",
        "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
        "        features = torch.tanh(features)\n",
        "        # There is only one output of self.w_v, so we remove the last\n",
        "        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n",
        "        # no. of queries, no. of key-value pairs)\n",
        "        scores = self.w_v(features).squeeze(-1)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        # Shape of values: (batch_size, no. of key-value pairs, value\n",
        "        # dimension)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c313394",
      "metadata": {
        "origin_pos": 44,
        "id": "4c313394"
      },
      "source": [
        "Let's [**see how `AdditiveAttention` works**]. In our toy example we pick queries, keys and values of size\n",
        "$(2, 1, 20)$, $(2, 10, 2)$ and $(2, 10, 4)$, respectively. This is identical to our choice for `DotProductAttention`, except that now the queries are $20$-dimensional. Likewise, we pick $(2, 6)$ as the valid lengths for the sequences in the minibatch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c1e66c95",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.983249Z",
          "iopub.status.busy": "2023-08-18T19:43:48.982715Z",
          "iopub.status.idle": "2023-08-18T19:43:48.993364Z",
          "shell.execute_reply": "2023-08-18T19:43:48.992407Z"
        },
        "origin_pos": 46,
        "tab": [
          "pytorch"
        ],
        "id": "c1e66c95"
      },
      "outputs": [],
      "source": [
        "queries = torch.normal(0, 1, (2, 1, 20))\n",
        "\n",
        "attention = AdditiveAttention(num_hiddens=8, dropout=0.1)\n",
        "attention.eval()\n",
        "d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f37c23",
      "metadata": {
        "origin_pos": 49,
        "id": "d0f37c23"
      },
      "source": [
        "When reviewing the attention function we see a behavior that is qualitatively quite similar to that of `DotProductAttention`. That is, only terms within the chosen valid length $(2, 6)$ are nonzero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bf7a330b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:48.996815Z",
          "iopub.status.busy": "2023-08-18T19:43:48.996248Z",
          "iopub.status.idle": "2023-08-18T19:43:49.212301Z",
          "shell.execute_reply": "2023-08-18T19:43:49.211395Z"
        },
        "origin_pos": 50,
        "tab": [
          "pytorch"
        ],
        "id": "bf7a330b",
        "outputId": "0c80e170-85cc-47c0-d40a-1adc824f647b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 250x250 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"187.07675pt\" height=\"103.438906pt\" viewBox=\"0 0 187.07675 103.438906\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-09-04T06:51:41.001772</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 103.438906 \nL 187.07675 103.438906 \nL 187.07675 0 \nL -0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 59.94 \nL 145.840625 59.94 \nL 145.840625 37.62 \nL 34.240625 37.62 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#p965e71ea4b)\">\n    <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAJsAAAAfCAYAAADwQL9CAAAAtElEQVR4nO3asQnCUBiF0RcF0wmiizmG26S0t3UMR3EBjYKKoDu8B1eC5/SX/xUfqdK9j8On1LqN1dNSSimrdf12vLTdfj3rt4u+6fR8u2vaT9Xs1w/gf4iNGLERIzZixEaM2IgRGzFiI0ZsxIiNGLERIzZixEaM2IgRGzHdYbmp/p/tdH00Hd/fz017psWXjRixESM2YsRGjNiIERsxYiNGbMSIjRixESM2YsRGjNiIERsxX65JEQNL/GsvAAAAAElFTkSuQmCC\" id=\"image928d0012f9\" transform=\"scale(1 -1) translate(0 -22.32)\" x=\"34.240625\" y=\"-37.62\" width=\"111.6\" height=\"22.32\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"md827cd0302\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#md827cd0302\" x=\"39.820625\" y=\"59.94\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(36.639375 74.538438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#md827cd0302\" x=\"95.620625\" y=\"59.94\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(92.439375 74.538438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Keys -->\n     <g transform=\"translate(78.371094 88.216563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 2694 \nL 3353 4666 \nL 4166 4666 \nL 1850 2491 \nL 4331 0 \nL 3500 0 \nL 1259 2247 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4b\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(60.576172 0)\"/>\n      <use xlink:href=\"#DejaVuSans-79\" transform=\"translate(122.099609 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(181.279297 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path id=\"md5f4936348\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#md5f4936348\" x=\"34.240625\" y=\"43.2\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(20.878125 46.999219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#md5f4936348\" x=\"34.240625\" y=\"54.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(20.878125 58.159219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Queries -->\n     <g transform=\"translate(14.798437 68.087031) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-51\" d=\"M 2522 4238 \nQ 1834 4238 1429 3725 \nQ 1025 3213 1025 2328 \nQ 1025 1447 1429 934 \nQ 1834 422 2522 422 \nQ 3209 422 3611 934 \nQ 4013 1447 4013 2328 \nQ 4013 3213 3611 3725 \nQ 3209 4238 2522 4238 \nz\nM 3406 84 \nL 4238 -825 \nL 3475 -825 \nL 2784 -78 \nQ 2681 -84 2626 -87 \nQ 2572 -91 2522 -91 \nQ 1538 -91 948 567 \nQ 359 1225 359 2328 \nQ 359 3434 948 4092 \nQ 1538 4750 2522 4750 \nQ 3503 4750 4090 4092 \nQ 4678 3434 4678 2328 \nQ 4678 1516 4351 937 \nQ 4025 359 3406 84 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-51\"/>\n      <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(78.710938 0)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(142.089844 0)\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(203.613281 0)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(244.726562 0)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(272.509766 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(334.033203 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 59.94 \nL 34.240625 37.62 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 145.840625 59.94 \nL 145.840625 37.62 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 59.94 \nL 145.840625 59.94 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 37.62 \nL 145.840625 37.62 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 152.815625 90.36 \nL 156.973625 90.36 \nL 156.973625 7.2 \nL 152.815625 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAYAAAB0CAYAAACmJkOCAAAAyElEQVR4nM2WSwrEMAxDXcj9zzqbblp/egG/gEII02WEJMt2Qq+6f2XNN6yyO7dhGQQQo1qLFY9ij30AB5Rz6B7uIqNQKkiKAUwuAyRV8e6SmjTxgAcy5An+qTkD+vrQNVgw15sI5qWXi909kLwCpTCgDPgB83dfuXpVFDAD3vakR18H3KEqWJKJVCBAyTEgSukBw2WP9thsJP0zOAEoFcjoz22EUQ5kyFVhwH5BZx4MkBQ2Ee7mhAEjX2E88sxRCluyYg4MquoD0XoQUGMmzEcAAAAASUVORK5CYII=\" id=\"image01ce98a0ea\" transform=\"scale(1 -1) translate(0 -83.52)\" x=\"152.64\" y=\"-6.48\" width=\"4.32\" height=\"83.52\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path id=\"mb8bb7f834f\" d=\"M 0 0 \nL 3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mb8bb7f834f\" x=\"156.973625\" y=\"90.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(163.973625 94.159219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mb8bb7f834f\" x=\"156.973625\" y=\"59.131442\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(163.973625 62.930661) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#mb8bb7f834f\" x=\"156.973625\" y=\"27.902884\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(163.973625 31.702103) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-34\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 152.815625 90.36 \nL 154.894625 90.36 \nL 156.973625 90.36 \nL 156.973625 7.2 \nL 154.894625 7.2 \nL 152.815625 7.2 \nL 152.815625 90.36 \nz\n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p965e71ea4b\">\n   <rect x=\"34.240625\" y=\"37.62\" width=\"111.6\" height=\"22.32\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\n",
        "                  xlabel='Keys', ylabel='Queries')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62fe2877",
      "metadata": {
        "origin_pos": 52,
        "id": "62fe2877"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this section we introduced the two key attention scoring functions: dot product and additive attention. They are effective tools for aggregating across sequences of variable length. In particular, the dot product attention is the mainstay of modern Transformer architectures. When queries and keys are vectors of different lengths,\n",
        "we can use the additive attention scoring function instead. Optimizing these layers is one of the key areas of advance in recent years. For instance, [NVIDIA's Transformer Library](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html) and Megatron :cite:`shoeybi2019megatron` crucially rely on efficient variants of the attention mechanism. We will dive into this in quite a bit more detail as we review Transformers in later sections.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Implement distance-based attention by modifying the `DotProductAttention` code. Note that you only need the squared norms of the keys $\\|\\mathbf{k}_i\\|^2$ for an efficient implementation.\n",
        "1. Modify the dot product attention to allow for queries and keys of different dimensionalities by employing a matrix to adjust dimensions.\n",
        "1. How does the computational cost scale with the dimensionality of the keys, queries, values, and their number? What about the memory bandwidth requirements?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex1:"
      ],
      "metadata": {
        "id": "wxTwiQYafxEg"
      },
      "id": "wxTwiQYafxEg"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def masked_softmax(X, valid_lens):\n",
        "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
        "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
        "    def _sequence_mask(X, valid_len, value=0):\n",
        "        maxlen = X.size(1)\n",
        "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                            device=X.device)[None, :] < valid_len[:, None]\n",
        "        X[~mask] = value\n",
        "        return X\n",
        "\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        # On the last axis, replace masked elements with a very large negative\n",
        "        # value, whose exponentiation outputs 0\n",
        "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
        "\n",
        "class DistanceAttention(nn.Module):\n",
        "    \"\"\"Scaled dot product attention.\"\"\"\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Shape of queries: (batch_size, no. of queries, d)\n",
        "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
        "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
        "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        d = queries.shape[-1]\n",
        "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "        key_norms = torch.sum(keys ** 2, dim=-1)/ math.sqrt(d) # (batch_size, num_keys)\n",
        "        scores = scores - 0.5*key_norms.unsqueeze(1) # (batch_size, num_queries, num_keys)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ],
      "metadata": {
        "id": "zWbWwAo9fzsk"
      },
      "id": "zWbWwAo9fzsk",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = torch.normal(0, 1, (2, 1, 2))\n",
        "keys = torch.normal(0, 1, (2, 10, 2))\n",
        "values = torch.normal(0, 1, (2, 10, 4))\n",
        "valid_lens = torch.tensor([2, 6])\n",
        "\n",
        "attention = DistanceAttention(dropout=0.5)\n",
        "attention.eval()\n",
        "attention(queries, keys, values, valid_lens).shape"
      ],
      "metadata": {
        "id": "YVcxRTUAhl2N",
        "outputId": "553be514-ff3d-4e05-a8bf-48c97fdfb93d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YVcxRTUAhl2N",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex2:"
      ],
      "metadata": {
        "id": "9TwnREyuiLHc"
      },
      "id": "9TwnREyuiLHc"
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffDimDotProductAttention(nn.Module):\n",
        "    \"\"\"Scaled dot product attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, dropout):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.LazyLinear(num_hiddens, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Shape of queries: (batch_size, no. of queries, d)\n",
        "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
        "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
        "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        queries = self.W_q(queries)\n",
        "        d = queries.shape[-1]\n",
        "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ],
      "metadata": {
        "id": "57_0MIYUiJW1"
      },
      "id": "57_0MIYUiJW1",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = torch.normal(0, 1, (2, 1, 10))\n",
        "keys = torch.normal(0, 1, (2, 10, 3))\n",
        "values = torch.normal(0, 1, (2, 10, 4))\n",
        "valid_lens = torch.tensor([2, 6])\n",
        "\n",
        "attention = DiffDimDotProductAttention(keys.shape[-1], dropout=0.5)\n",
        "attention.eval()\n",
        "attention(queries, keys, values, valid_lens).shape"
      ],
      "metadata": {
        "id": "B5XwCCfHij87",
        "outputId": "0e633684-b743-490c-d96b-f28538bfcb8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "B5XwCCfHij87",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37239f57",
      "metadata": {
        "origin_pos": 54,
        "tab": [
          "pytorch"
        ],
        "id": "37239f57"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1064)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}